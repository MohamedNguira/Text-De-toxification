{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import src.data.preprocess1 as custom_preprocess\n",
    "\n",
    "# Get the current working directory\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Define the directory where the dataset is located\n",
    "DATASET_DIR = (Path(ROOT_DIR).parent / 'data').resolve()\n",
    "\n",
    "# Create the full path for the dataset file\n",
    "file_path = (DATASET_DIR / 'filtered.tsv').resolve()\n",
    "\n",
    "# Initialize the current location as the root directory\n",
    "current_location = ROOT_DIR\n",
    "\n",
    "# Traverse up the directory tree until 'src' is found in the directory names\n",
    "while not any('src' in entry.name for entry in os.scandir(current_location)):\n",
    "    current_location = Path(current_location).parent.resolve()\n",
    "\n",
    "import sys\n",
    "\n",
    "# Set the parent directory to the current location\n",
    "PARENT_DIRECTORY = current_location\n",
    "\n",
    "# Add the parent directory to the system path for module imports\n",
    "sys.path.append(str(current_location))\n",
    "\n",
    "# Define the path for the processed file\n",
    "processed_file = os.path.join(DATASET_DIR, 'firstprocess.csv')\n",
    "\n",
    "# Perform custom preprocessing on the data\n",
    "processed_data = custom_preprocess.process_everything(fixed_data_file=processed_file, save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process text by applying a series of custom preprocessing steps\n",
    "def process_text(text):\n",
    "    # Chain custom preprocessing functions to the input text\n",
    "    return custom_preprocess.delete_spaces(custom_preprocess.delete_extra(custom_preprocess.to_lowercase(text)))\n",
    "\n",
    "# Function to process a batch of text data using a specified spaCy NLP model\n",
    "# It applies the custom preprocessing to both 'source' and 'target' keys in the batch\n",
    "from typing import Dict\n",
    "def process_batch(batch: Dict, nlp):\n",
    "    # Use a dictionary comprehension to apply process_text function to each item in the 'source' and 'target' values\n",
    "    return dict([(key, [process_text(item) for item in value]) for key, value in {\"source\": custom_preprocess.universal_batch(batch['source'], nlp), \"target\": custom_preprocess.universal_batch(batch['target'], nlp)}.items()])\n",
    "\n",
    "# Load the spaCy NLP model for English\n",
    "import spacy\n",
    "nlp_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Apply the process_batch function to a batch of data using the loaded NLP model\n",
    "processed_data = processed_data.map(lambda batch: process_batch(batch, nlp_model), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter processed_data to retain only items where 'source' and 'target' are both strings\n",
    "processed_data = processed_data.filter(lambda s: (isinstance(s['source'], str) and isinstance(s['target'], str)))\n",
    "\n",
    "# Save the preprocessed data to a CSV file named 'everything_prepared.csv' in the specified directory\n",
    "import os\n",
    "processed_data.to_csv(os.path.join(DATASET_DIR, 'everything_prepared.csv'), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
