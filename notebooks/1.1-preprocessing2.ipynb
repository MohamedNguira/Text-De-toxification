{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import T5TokenizerFast, AutoModelForSeq2SeqLM\n",
    "import os\n",
    "from src.models.train.model1.train2 import toxicity_classication as tc\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Define the directory where the dataset is located\n",
    "DATASET_DIR = (Path(ROOT_DIR).parent / 'data').resolve()\n",
    "\n",
    "# Create the full path for the dataset file\n",
    "file_path = (DATASET_DIR / 'filtered.tsv').resolve()\n",
    "\n",
    "# Initialize the current location as the root directory\n",
    "current_location = ROOT_DIR\n",
    "\n",
    "# Traverse up the directory tree until 'src' is found in the directory names\n",
    "while not any('src' in entry.name for entry in os.scandir(current_location)):\n",
    "    current_location = Path(current_location).parent.resolve()\n",
    "\n",
    "import sys\n",
    "\n",
    "# Set the parent directory to the current location\n",
    "PARENT_DIRECTORY = current_location\n",
    "\n",
    "# Add the parent directory to the system path for module imports\n",
    "sys.path.append(str(current_location))\n",
    "\n",
    "\n",
    "current = ROOT_DIR\n",
    "while 'src' not in os.listdir(current):\n",
    "    current = Path(current).parent\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(current))\n",
    "\n",
    "DATA_FOLDER = os.path.join(Path(current).parent, 'data')\n",
    "data_path = os.path.join(DATA_FOLDER, 'firstprocess.csv')\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=data_path).remove_columns(['lenght_diff','source_tox','similarity', 'target_tox'].reverse())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint model as 't5-small'\n",
    "MODEL_CHECKPOINT = 't5-small'\n",
    "\n",
    "# Check if a GPU is available; assign 'cuda' if true, else use 'cpu'\n",
    "DEVICE_TYPE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Create a T5 tokenizer instance based on the specified checkpoint\n",
    "TOKENIZER_INSTANCE = T5TokenizerFast.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# Load a pre-trained sequence-to-sequence model from the specified checkpoint and transfer it to the chosen device\n",
    "SEQUENCE_TO_SEQUENCE_MODEL = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT).to(DEVICE_TYPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefix to be added to each text in the task\n",
    "TASK_PREFIX = 'summarize: '\n",
    "\n",
    "# Function to prepare data by adding the task prefix to each text in the specified batch\n",
    "def prepare_data(batch, split: str ='source'):\n",
    "    # Create a list with each text in the batch prefixed with the specified task prefix\n",
    "    tok_batch = [TASK_PREFIX + s for s in batch[split]]\n",
    "    \n",
    "    # Tokenize the prepared batch with truncation enabled\n",
    "    return TOKENIZER_INSTANCE(tok_batch, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a subset of data from the 'train' split in the dataset and shuffle it with a seed of 69\n",
    "sampled_data = dataset['train'].shuffle(seed=69)[:10]\n",
    "\n",
    "# Tokenize the 'source' text in the sampled data using the prepare_data function\n",
    "source_tokenized_data = sampled_data.map(prepare_data, batched=True).remove_columns(['source', 'target'].reverse())\n",
    "\n",
    "# Tokenize the 'target' text in the sampled data using the prepare_data function with split='target'\n",
    "target_tokenized_data = sampled_data.map(lambda x: prepare_data(x, split='target'), batched=True).remove_columns(['source', 'target', 'attention_mask'].reverse())\n",
    "\n",
    "# Display the first 10 samples of the tokenized 'source' data\n",
    "print(source_tokenized_data[:10])\n",
    "\n",
    "# Display the first 10 samples of the tokenized 'target' data\n",
    "print(target_tokenized_data[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data collator with padding using the specified tokenizer\n",
    "collator_with_padding = DataCollatorWithPadding(tokenizer=TOKENIZER_INSTANCE)\n",
    "\n",
    "# Create a DataLoader for the tokenized 'source' data with batch size 16, no shuffling, and using the defined data collator\n",
    "source_data_loader = DataLoader(dataset=source_tokenized_data, batch_size=16, shuffle=False, collate_fn=collator_with_padding)\n",
    "6\n",
    "# Create a DataLoader for the tokenized 'target' data with batch size 16, no shuffling, and using the defined data collator\n",
    "target_data_loader = DataLoader(dataset=target_tokenized_data, batch_size=1, shuffle=False, collate_fn=collator_with_padding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to build a dataset by generating summaries using the model\n",
    "def build_dataset():\n",
    "    # Iterate through batches of tokenized 'source' and 'target' data\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for source_batch, target_batch in zip(source_data_loader, target_data_loader):\n",
    "        # Move the batch to the specified device\n",
    "        model_batch = {k: v.to(DEVICE_TYPE) for k, v in source_batch.items()}\n",
    "        print(\"Finished iteration \", i)\n",
    "        # Generate summaries using the model\n",
    "        output = SEQUENCE_TO_SEQUENCE_MODEL.generate(**model_batch)\n",
    "        output_decoded = TOKENIZER_INSTANCE.batch_decode(output, skip_special_tokens=True)\n",
    "        \n",
    "        # Decode the source and target batches\n",
    "        source = TOKENIZER_INSTANCE.batch_decode(source_batch['input_ids'], skip_special_tokens=True)\n",
    "        i+=1\n",
    "        target = TOKENIZER_INSTANCE.batch_decode(target_batch['input_ids'], skip_special_tokens=True)\n",
    "\n",
    "        # Perform toxic classification on the generated summaries\n",
    "        summary_toxicity = tc.toxic_classification(output_decoded)\n",
    "\n",
    "        # Yield dictionaries containing source, target, summary, and summary toxicity information\n",
    "        for summary, source_text, target_text, toxicity in zip(output_decoded, source, target, summary_toxicity):\n",
    "            j += 1\n",
    "            yield {\"source\": re.sub(TASK_PREFIX, \"\", source_text), \"target\": re.sub(TASK_PREFIX, \"\", target_text), \"sum\": summary, \"sumtox\": toxicity}\n",
    "\n",
    "# Create a dataset from the generator function\n",
    "model1_dataset = Dataset.from_generator(build_dataset)\n",
    "\n",
    "# Save the generated dataset to a CSV file in the specified data folder\n",
    "model1_dataset.to_csv(os.path.join(DATA_FOLDER, 'model1.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read the CSV file 'model1.csv' located in the specified data folder into a Pandas DataFrame\n",
    "model1_dataframe = pd.read_csv(os.path.join(DATA_FOLDER, 'model1.csv'))\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "model1_dataframe.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
